OK
OK
Query ID = sankuai_20171031192605_d17bbfbb-0772-4549-b87f-178cbac99f40
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1505297521386_3310, Tracking URL = http://gh-data-hdp-qa04.corp.sankuai.com:8088/proxy/application_1505297521386_3310/
Kill Command = /opt/meituan/hadoop-sentry-qa/bin/hadoop job  -kill job_1505297521386_3310
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
Ended Job = job_1505297521386_3310
Stage-3 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
Stage-4 is filtered out by condition resolver.
Moving data to: viewfs://hadoop-meituan-test/user/hive/warehouse/encrypt_db4data.db/.hive-staging_hive_2017-10-31_19-26-05_703_2773878941243151364-1/-ext-10000
Loading data to table encrypt_db4data.session_test
2 Rows loaded to encrypt_db4data.session_test
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.12 sec   HDFS Read: 5152 HDFS Write: 150 SUCCESS
OK
Query ID = sankuai_20171031192629_cd6299d3-b418-4312-824e-3ac5c4a18889
Total jobs = 1
Execution log at: /var/sankuai/logs/hive1.2/sankuai/hive.log
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1505297521386_3311, Tracking URL = http://gh-data-hdp-qa04.corp.sankuai.com:8088/proxy/application_1505297521386_3311/
Kill Command = /opt/meituan/hadoop-sentry-qa/bin/hadoop job  -kill job_1505297521386_3311
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
Ended Job = job_1505297521386_3311
Moving data to: viewfs://hadoop-meituan-test/user/hive/warehouse/encrypt_db4data.db/sessionization_step_one_origins_tgt_encrypted
2 Rows loaded to encrypt_db4data.sessionization_step_one_origins_t
MapReduce Jobs Launched: 
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.21 sec   HDFS Read: 16714 HDFS Write: 62 SUCCESS
OK
Query ID = sankuai_20171031192703_bbba11ec-0e90-472a-8781-414afba6a32d
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1505297521386_3312, Tracking URL = http://gh-data-hdp-qa04.corp.sankuai.com:8088/proxy/application_1505297521386_3312/
Kill Command = /opt/meituan/hadoop-sentry-qa/bin/hadoop job  -kill job_1505297521386_3312
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Ended Job = job_1505297521386_3312
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.38 sec   HDFS Read: 8156 HDFS Write: 2 SUCCESS
OK
c0
2
OK
col_name	data_type	comment
ENCRYPTED_NAME_73582_encrypt_ssoo_user_id	string              	ENCRYPTED_COMMENT_19961
ENCRYPTED_NAME_40337_encrypt_ssoo_pageview_id	string              	ENCRYPTED_COMMENT_51714
OK
OK
