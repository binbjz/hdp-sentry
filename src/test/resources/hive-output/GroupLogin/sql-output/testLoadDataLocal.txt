OK
Loading data to table mart_waimai.collecttest
OK
FAILED: SemanticException Line 0:-1 Expression not in GROUP BY key 'countVal'
Query ID = sankuai_20171031213305_b6695a61-56f9-4aea-a146-e5b4b4f99835
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1505297521386_3448, Tracking URL = http://gh-data-hdp-qa04.corp.sankuai.com:8088/proxy/application_1505297521386_3448/
Kill Command = /opt/meituan/hadoop-sentry-qa/bin/hadoop job  -kill job_1505297521386_3448
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Ended Job = job_1505297521386_3448
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.14 sec   HDFS Read: 7253 HDFS Write: 18 SUCCESS
OK
eleven	2
twelve	2
Query ID = sankuai_20171031213334_95d008dc-5635-4115-b8f8-1a2a5cc87f3e
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1505297521386_3449, Tracking URL = http://gh-data-hdp-qa04.corp.sankuai.com:8088/proxy/application_1505297521386_3449/
Kill Command = /opt/meituan/hadoop-sentry-qa/bin/hadoop job  -kill job_1505297521386_3449
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Ended Job = job_1505297521386_3449
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.97 sec   HDFS Read: 6937 HDFS Write: 2 SUCCESS
OK
4
OK
FAILED: SemanticException Line 0:-1 Expression not in GROUP BY key 'countVal'
Query ID = sankuai_20171031213407_360a2ff7-46d6-4bf7-9b25-fa7313add459
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1505297521386_3450, Tracking URL = http://gh-data-hdp-qa04.corp.sankuai.com:8088/proxy/application_1505297521386_3450/
Kill Command = /opt/meituan/hadoop-sentry-qa/bin/hadoop job  -kill job_1505297521386_3450
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Ended Job = job_1505297521386_3450
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.59 sec   HDFS Read: 7315 HDFS Write: 18 SUCCESS
OK
eleven	2
twelve	2
Query ID = sankuai_20171031213433_3f13c765-d64b-4196-941e-33bcd5f74e68
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1505297521386_3451, Tracking URL = http://gh-data-hdp-qa04.corp.sankuai.com:8088/proxy/application_1505297521386_3451/
Kill Command = /opt/meituan/hadoop-sentry-qa/bin/hadoop job  -kill job_1505297521386_3451
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Ended Job = job_1505297521386_3451
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.05 sec   HDFS Read: 6894 HDFS Write: 2 SUCCESS
OK
4
